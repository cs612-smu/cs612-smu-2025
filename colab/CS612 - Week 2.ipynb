{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPm1sALG4/wu7olpliCj3lb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Clone the repository"],"metadata":{"id":"V2fKZxaFjHWA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIEqRfSqjCit"},"outputs":[],"source":["try:\n","    ! git clone https://github.com/cs612-smu/cs612-smu-2025 CS612_SMU\n","    HOME_DIR = \"./CS612_SMU/week2/\"\n","except:\n","    print('Already clone!!!')"]},{"cell_type":"markdown","source":["Exercise 2a: This following is an implementation of the FGSM attach that is untargeted."],"metadata":{"id":"fYkrw92KjkBy"}},{"cell_type":"code","source":["# This is an implementation of FGSM, an untargeted attack\n","\n","\n","import torch\n","\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import ast\n","\n","\n","class MNISTNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(784, 10)\n","        self.fc2 = nn.Linear(10, 10)\n","        self.fc3 = nn.Linear(10, 10)\n","        self.fc4 = nn.Linear(10, 10)\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","        x = F.relu(x)\n","        x = self.fc4(x)\n","        output = x # cross entropy in pytorch already includes softmax\n","        return output\n","\n","\n","def load_model(model_class, name):\n","    model = model_class()\n","    model.load_state_dict(torch.load(name))\n","\n","    return model\n","\n","\n","def denormalize(x):\n","    x = (x * 255).astype('uint8')\n","    x = x.reshape(28,28)\n","\n","    return x\n","\n","\n","def display(x, y, x_adv, y_adv):\n","    x = denormalize(x)\n","    x_adv = denormalize(x_adv)\n","\n","    fig, ax = plt.subplots(1, 2)\n","\n","    ax[0].set(title='Original. Label is {}'.format(y))\n","    ax[1].set(title='Adv. sample. Label is {}'.format(y_adv))\n","\n","    ax[0].imshow(x, cmap='gray')\n","    ax[1].imshow(x_adv, cmap='gray')\n","\n","    plt.show()\n","\n","\n","def fgsm(model, x, y, eps): #x is the sample, y is its original label; eps is the attacking budget\n","    x_adv = x.detach().clone()\n","    x_adv.requires_grad = True\n","\n","    pred = model(x_adv)\n","    loss = F.cross_entropy(pred, y)\n","\n","    loss.backward()\n","\n","    grad_data = x_adv.grad.data\n","    #The following line of code is the essence of FGSM.\n","    #That is, we update x_adv (initially an exact copy of x) by updating its pixel values according to the gradient.\n","    #Note that grad_data is the gradient obtained based on the loss above.\n","    #The following line thus effectively increases the loss of the original label.\n","    x_adv = torch.clamp(x_adv + eps * grad_data.sign(), 0, 1).detach()\n","\n","    pred_adv = model(x_adv) #get the prediction of x'\n","    y_adv = pred_adv.argmax(1) #get the label of x'\n","\n","    if y_adv != y:\n","        x = x.detach().numpy().reshape(-1)\n","        x_adv = x_adv.detach().numpy().reshape(-1)\n","\n","        y, y_adv = y.item(), y_adv.item()\n","\n","        print('Attack successful: Found an adversarial sample!\\n')\n","\n","        #print('pred adv = {}'.format(pred_adv.detach().numpy().reshape(-1)))\n","        #print('lbl adv = {}\\n'.format(y_adv))\n","\n","        display(x, y, x_adv, y_adv)\n","        return True\n","    else:\n","        print('Attack unsuccessful.\\n')\n","        return False\n","\n","\n","model = load_model(MNISTNet, HOME_DIR + 'exercise2/mnist.pt')\n","num_adv, eps = 0, 0.05 #vary eps to see the effect\n","\n","labels = np.array(ast.literal_eval(open(HOME_DIR + 'exercise2/toattack/labels.txt', 'r').readline()))\n","\n","num_attack = 5\n","print('Initiating FGSM untargeted Attack on {} images.\\n'.format(num_attack ))\n","for i in range(num_attack):\n","\n","    file_name = HOME_DIR + 'exercise2/toattack/data' + str(i) + '.txt'\n","    x = np.array(ast.literal_eval(open(file_name, 'r').readline()))\n","    x = torch.Tensor(x)\n","    y = torch.Tensor([labels[i]]).type(torch.LongTensor)\n","\n","    pred = model(x)\n","    #print('pred img = {}'.format(pred.detach().numpy().reshape(-1)))\n","    #print('lbl imp = {}\\n'.format(y.item()))\n","\n","    if fgsm(model, x, y, eps): num_adv += 1\n","\n","print('{} of the {} attacks are successful.'.format(num_adv, num_attack))\n"],"metadata":{"id":"aTu-f2qNjlRD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exercise 2b: In this exercise, you are asked to modify the FGSM attack method so that it becomes a targeted attack."],"metadata":{"id":"USYQWLZgk1XU"}},{"cell_type":"code","source":["# This is an implementation of FGSM, an untargeted attack\n","\n","\n","import torch\n","\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import ast\n","\n","\n","class MNISTNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(784, 10)\n","        self.fc2 = nn.Linear(10, 10)\n","        self.fc3 = nn.Linear(10, 10)\n","        self.fc4 = nn.Linear(10, 10)\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","        x = F.relu(x)\n","        x = self.fc4(x)\n","        output = x # cross entropy in pytorch already includes softmax\n","        return output\n","\n","\n","def load_model(model_class, name):\n","    model = model_class()\n","    model.load_state_dict(torch.load(name))\n","\n","    return model\n","\n","\n","def denormalize(x):\n","    x = (x * 255).astype('uint8')\n","    x = x.reshape(28,28)\n","\n","    return x\n","\n","\n","def display(x, y, x_adv, y_adv):\n","    x = denormalize(x)\n","    x_adv = denormalize(x_adv)\n","\n","    fig, ax = plt.subplots(1, 2)\n","\n","    ax[0].set(title='Original. Label is {}'.format(y))\n","    ax[1].set(title='Adv. sample. Label is {}'.format(y_adv))\n","\n","    ax[0].imshow(x, cmap='gray')\n","    ax[1].imshow(x_adv, cmap='gray')\n","\n","    plt.show()\n","\n","\n","def fgsm(model, x, y, eps, target): # pass the target label as parameter for the targeted attack\n","    # x is the image sample (Tensor), y is the original label (Tensor)\n","    target = torch.Tensor([target]).type(torch.LongTensor)\n","    if y == target:\n","        print('The sample is already classified as the target! Skip the sample.\\n')\n","        return False\n","\n","    x_adv = x.detach().clone()\n","    x_adv.requires_grad = True\n","\n","    pred = model(x_adv)\n","    #this line has been modified for a targeted attack already\n","    loss = F.cross_entropy(pred, target)\n","\n","    loss.backward()\n","\n","    grad_data = x_adv.grad.data\n","\n","    #TODO: add one line here to conduct a targeted attack according to your formulation of the optimization problem.\n","    #You can refer to the corresponding line in the untargeted attack for inspiration.\n","    #Note that grad_data is the gradient obtained based on the loss of the target.\n","\n","    #the following code checks whether the attack is successful.\n","    pred_adv = model(x_adv)\n","    y_adv = pred_adv.argmax(1)\n","\n","    # if y_adv != y: # modify the condition to catch succesful targeted attacks\n","    if y_adv == target:\n","        x = x.detach().numpy().reshape(-1)\n","        x_adv = x_adv.detach().numpy().reshape(-1)\n","\n","        y, y_adv = y.item(), y_adv.item()\n","\n","        print('Attack successful: Found an adversarial sample!\\n')\n","\n","        #print('pred adv = {}'.format(pred_adv.detach().numpy().reshape(-1)))\n","        #print('lbl adv = {}\\n'.format(y_adv))\n","\n","        display(x, y, x_adv, y_adv)\n","        return True\n","    else:\n","        print('Attack unsuccessful.\\n')\n","        return False\n","\n","\n","model = load_model(MNISTNet, HOME_DIR + 'exercise2/mnist.pt')\n","num_adv, eps = 0, 0.2\n","\n","labels = np.array(ast.literal_eval(open(HOME_DIR + 'exercise2/toattack/labels.txt', 'r').readline()))\n","\n","num_attack = 5\n","target = 7\n","print('Initiating FGSM targeted Attack on {} images with target {}.\\n'.format(num_attack, target))\n","\n","for i in range(num_attack):\n","    print('Attacking a new image.')\n","    file_name = HOME_DIR + 'exercise2/toattack/data' + str(i) + '.txt'\n","    x = np.array(ast.literal_eval(open(file_name, 'r').readline()))\n","    x = torch.Tensor(x)\n","    y = torch.Tensor([labels[i]]).type(torch.LongTensor)\n","\n","    pred = model(x)\n","    #print('pred img = {}'.format(pred.detach().numpy().reshape(-1)))\n","    #print('lbl imp = {}\\n'.format(y.item()))\n","\n","    if fgsm(model, x, y, eps, target): num_adv += 1\n","\n","print('{} of the {} attacks are successful.'.format(num_adv, num_attack))\n"],"metadata":{"id":"fUsZqkGhk2iD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exercise 3: In this exercise, we aim to see whether an adversaral sample is sensitive to noises. That is, we evaluate whether adding a bit of noise may \"remove\" the adversarial effect of an adversarial sample."],"metadata":{"id":"6ECtl7RsHtRb"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import ast\n","\n","\n","class MNISTNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(784, 10)\n","        self.fc2 = nn.Linear(10, 10)\n","        self.fc3 = nn.Linear(10, 10)\n","        self.fc4 = nn.Linear(10, 10)\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","        x = F.relu(x)\n","        x = self.fc4(x)\n","        output = x # cross entropy in pytorch already includes softmax\n","        return output\n","\n","\n","def load_model(model_class, name):\n","    model = model_class()\n","    model.load_state_dict(torch.load(name))\n","\n","    return model\n","\n","\n","def denormalize(x):\n","    x = (x * 255).astype('uint8')\n","    x = x.reshape(28,28)\n","\n","    return x\n","\n","\n","def display(x, y, x_adv, y_adv):\n","    x = denormalize(x)\n","    x_adv = denormalize(x_adv)\n","\n","    fig, ax = plt.subplots(1, 2)\n","\n","    ax[0].set(title='Original. Label is {}'.format(y))\n","    ax[1].set(title='Adv. sample. Label is {}'.format(y_adv))\n","\n","    ax[0].imshow(x, cmap='gray')\n","    ax[1].imshow(x_adv, cmap='gray')\n","\n","    plt.show()\n","\n","# x is the image sample (Tensor), y is the original label (Tensor)\n","def attack(model, x, y):\n","    #The following shows one way of introducing some minor modification to\n","    # the adversarial example, i.e, by adding some uniform noise.\n","    x_adv = x.detach().clone()\n","    noise = torch.Tensor(np.random.uniform(-0.01, 0.01, (1,1,28,28)))\n","    x_adv = torch.clamp(x_adv + noise, 0, 1)\n","\n","    #TODO: Modify the scale of the noise in the above method and note the\n","    #number of label changes (repeat each setting for at least 5 times and\n","    # take the average to reduce the effect of randomness);\n","\n","    pred_adv = model(x_adv)\n","    y_adv = pred_adv.argmax(1)\n","\n","    if y_adv != y:\n","        print('The label of the adversarial sample changes after adding the noise.')\n","        x = x.detach().numpy().reshape(-1)\n","        x_adv = x_adv.detach().numpy().reshape(-1)\n","\n","        y, y_adv = y.item(), y_adv.item()\n","\n","        display(x, y, x_adv, y_adv)\n","        return True\n","    else:\n","        print('The label of the adversarial sample remains after adding the noise.')\n","        return False\n","\n","\n","model = load_model(MNISTNet, HOME_DIR + 'exercise3/mnist.pt')\n","changes = 0\n","\n","num_attack = 20\n","for i in range(num_attack):\n","    file_name = HOME_DIR + 'exercise3/toattack_adv/adv_data' + str(i) + '.txt'\n","    x = np.array(ast.literal_eval(open(file_name, 'r').readline()))\n","    x = torch.Tensor(x)\n","\n","    pred = model(x)\n","    y = pred.argmax(1)\n","    #print('pred img = {}'.format(pred.detach().numpy().reshape(-1)))\n","\n","    if attack(model, x, y): changes += 1\n","\n","    print('\\nTrying a new adversarial sample.')\n","\n","print('After adding the noise, the number of adversarial samples whose label changes is {}.\\n'.format(changes))\n"],"metadata":{"id":"dt4U5woGHz6C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exercise 4: In this exercise, we aim to see whether adversarial training with FGSM adversarial attack is effective with FGSM adversarial attacks."],"metadata":{"id":"z_Vr_qwPIIeC"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import ast\n","\n","class MNISTNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(784, 10)\n","        self.fc2 = nn.Linear(10, 10)\n","        self.fc3 = nn.Linear(10, 10)\n","        self.fc4 = nn.Linear(10, 10)\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","        x = F.relu(x)\n","        x = self.fc4(x)\n","        output = x # cross entropy in pytorch already includes softmax\n","        return output\n","\n","def load_model(model_class, name):\n","    model = model_class()\n","    model.load_state_dict(torch.load(name, map_location=torch.device('cpu')))\n","    return model\n","\n","def denormalize(x):\n","    x = (x * 255).astype('uint8')\n","    x = x.reshape(28,28)\n","    return x\n","\n","\n","def display(x, y, x_adv, y_adv):\n","    x = denormalize(x)\n","    x_adv = denormalize(x_adv)\n","    fig, ax = plt.subplots(1, 2)\n","    ax[0].set(title='Original. Label is {}'.format(y))\n","    ax[1].set(title='Adv. sample. Label is {}'.format(y_adv))\n","    ax[0].imshow(x, cmap='gray')\n","    ax[1].imshow(x_adv, cmap='gray')\n","    plt.show()\n","\n","#The following implements FGSM attack.\n","#x is the image sample (Tensor), y is the original label (Tensor)\n","def fgsm(model, x, y, eps):\n","    x_adv = x.detach().clone()\n","    x_adv.requires_grad = True\n","    pred = model(x_adv)\n","    loss = F.cross_entropy(pred, y)\n","    loss.backward()\n","    grad_data = x_adv.grad.data\n","    x_adv = torch.clamp(x_adv + eps * grad_data.sign(), 0, 1).detach()\n","\n","    pred_adv = model(x_adv)\n","    y_adv = pred_adv.argmax(1)\n","\n","    if y_adv != y:\n","        x = x.detach().numpy().reshape(-1)\n","        x_adv = x_adv.detach().numpy().reshape(-1)\n","\n","        y, y_adv = y.item(), y_adv.item()\n","\n","        print('Attack success: Found an adversarial sample!\\n')\n","        #print('pred adv = {}'.format(pred_adv.detach().numpy().reshape(-1)))\n","        #print('lbl adv = {}\\n'.format(y_adv))\n","        display(x, y, x_adv, y_adv)\n","        return True\n","    else:\n","        print('Attack unsucessful.\\n')\n","        return False\n","\n","# model = load_model(MNISTNet, HOME_DIR + 'exercise4/mnist.pt')\n","\n","#TODO：Change the model by replacing the above line with the following one.\n","model = load_model(MNISTNet, HOME_DIR + 'exercise4/mnist_robust.pt')\n","#TODO: Vary the eps below to see the effect with different attacking budget.\n","\n","# The robust model was trained using adversarial training with FGSM to improve robustness.\n","# Key details:\n","# - FGSM configuration: epsilon = 0.03.\n","# - Training involved both clean and adversarial examples.\n","# - Adversarial examples were generated on-the-fly during training using the gradient of the loss.\n","# - Model: Pretrained `MNISTNet` with 4 fully connected layers and ReLU activations.\n","# - Optimizer: Adam (lr = 0.001), Batch size: 64, Epochs: 30.\n","\n","num_adv, eps = 0, 0.05\n","\n","labels = np.array(ast.literal_eval(open(HOME_DIR + 'exercise4/toattack/labels.txt', 'r').readline()))\n","\n","num_attack = 20\n","for i in range(num_attack):\n","    print('Trying a new image.')\n","    file_name = HOME_DIR + 'exercise4/toattack/data' + str(i) + '.txt'\n","    x = np.array(ast.literal_eval(open(file_name, 'r').readline()))\n","    x = torch.Tensor(x)\n","    y = torch.Tensor([labels[i]]).type(torch.LongTensor)\n","\n","    pred = model(x)\n","\n","    if fgsm(model, x, y, eps): num_adv += 1\n","\n","print('Out of 20 samples, {} are attacked successfully.'.format(num_adv))\n"],"metadata":{"id":"pp-SAn33IP1z"},"execution_count":null,"outputs":[]}]}